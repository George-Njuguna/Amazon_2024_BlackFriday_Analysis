{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Deals URL\n",
    "URL = 'https://www.amazon.com/gp/goldbox?ref_=nav_cs_gb&discounts-widget=%2522%257B%255C%2522state%255C%2522%253A%257B%255C%2522refinementFilters%255C%2522%253A%257B%257D%257D%252C%255C%2522version%255C%2522%253A1%257D%2522'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dictonary to save the scraped data\n",
    "data2 =  { 'Product_ID' : [] ,'Product_Name' : [] ,'Category' : [] , 'Product_Link' : [] ,'Discounted_Price' : [] , 'Actual_Price' : [] , 'Rating' : [] , 'Rating_Count' : [] , 'Reviewers' : [] , 'Reviews' : []   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "productID_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # scrolling in small increments\n",
    "increment = 0.03  # Scroll by 3% of the remaining page height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "     # opening the page \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get( URL )\n",
    "    driver.maximize_window()\n",
    "    driver.execute_script(\"document.body.style.zoom='80%'\")\n",
    "\n",
    "    while True:\n",
    "    \n",
    "         # Create an explicit wait object\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "\n",
    "         # Wait for an element to be present\n",
    "        element = wait.until(EC.presence_of_all_elements_located(( By.CSS_SELECTOR, 'a[data-testid=\"product-card-link\"]') ))\n",
    "\n",
    "        html1 = driver.page_source\n",
    "\n",
    "         # Creating a soup object \n",
    "        main_soup = BeautifulSoup( html1 , 'lxml' )\n",
    "\n",
    "         # getting the div\n",
    "        div_attr = main_soup.select('div:has(a[data-testid=\"product-card-link\"])') \n",
    "\n",
    "        for elements in div_attr:\n",
    "\n",
    "            Product_id = elements.get('data-asin')\n",
    "\n",
    "            if Product_id in productID_set:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                 # Updating the Product Id set\n",
    "                productID_set.add(Product_id)\n",
    "\n",
    "                data2['Product_ID'].append(Product_id)\n",
    "\n",
    "                 # Find the a element within this div\n",
    "                a_element = elements.select_one('a[data-testid=\"product-card-link\"]')\n",
    "    \n",
    "                # Get the href attribute from the a element\n",
    "                if a_element:\n",
    "                     # Getting the link and adding it to the data2\n",
    "                    link = a_element.get('href')\n",
    "                    data2['Product_Link'].append(link)\n",
    "\n",
    "                    driver.get(link)\n",
    "                    sleep(1)\n",
    "                \n",
    "                     # Getting the data using bs4\n",
    "                    title = wait.until(EC.presence_of_element_located(( By.CSS_SELECTOR, 'span#productTitle') ))\n",
    "\n",
    "                    if title:\n",
    "                        # Getting the html \n",
    "                        html = driver.page_source\n",
    "\n",
    "                         # Beautiful Soup object\n",
    "                        soup_2 = BeautifulSoup( html , 'lxml' )\n",
    "                    \n",
    "                        # Getting the  actual prices\n",
    "                        actual_price_attr = soup_2.select('span.aok-relative span.a-size-small.aok-offscreen')\n",
    "\n",
    "\n",
    "                        if  actual_price_attr:\n",
    "                   \n",
    "                            actual = actual_price_attr[0].get_text().strip()\n",
    "\n",
    "                            data2['Actual_Price'].append(actual)\n",
    "                        \n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Actual_Price'].append('')\n",
    "\n",
    "                         # Discounted price\n",
    "                        discounted_price_attr = soup_2.select('span.a-price.aok-align-center span.a-offscreen')\n",
    "\n",
    "                        if  discounted_price_attr:\n",
    "                   \n",
    "                            discount = discounted_price_attr[0].get_text().strip()\n",
    "\n",
    "                            data2['Discounted_Price'].append(discount)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Discounted_Price'].append('')\n",
    "\n",
    "                         # Getting the Product_name\n",
    "                        name_attr = soup_2.select( 'span#productTitle') \n",
    "\n",
    "                        if name_attr:\n",
    "\n",
    "                            name = name_attr[0].get_text().strip().split(',')\n",
    "                            data2['Product_Name'].append(name[0])\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Product_Name'].append('')\n",
    "\n",
    "                         # Getting the Category\n",
    "                        category_attr = soup_2.select( 'span.a-list-item a[href]' )\n",
    "\n",
    "                        if category_attr:\n",
    "\n",
    "                            category = category_attr[0].get_text().strip() + '$' + category_attr[1].get_text().strip()\n",
    "                            data2['Category'].append(category)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Category'].append('')\n",
    "\n",
    "\n",
    "\n",
    "                         # Getting rating and rating count \n",
    "                        rating_attr = soup_2.select('span[data-hook=\"rating-out-of-text\"]')\n",
    "\n",
    "                        if rating_attr:\n",
    "                        \n",
    "                            rating = rating_attr[0].get_text().strip()\n",
    "                            data2['Rating'].append(rating)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Rating'].append('')\n",
    "\n",
    "                        rating_count_attr = soup_2.select('span[data-hook=\"total-review-count\"]')\n",
    "\n",
    "                        if rating_count_attr:\n",
    "\n",
    "                            count = rating_count_attr[0].get_text().strip()\n",
    "                            data2['Rating_Count'].append(count)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Rating_Count'].append('')\n",
    "\n",
    "\n",
    "                         # Getting the reviewer names \n",
    "                        reviewer_attr = soup_2.select( 'span.a-profile-name' )\n",
    "\n",
    "                        reviewers = []\n",
    "\n",
    "                        if reviewer_attr:\n",
    "\n",
    "                            for i in reviewer_attr:\n",
    "                                cleaned_text = i.get_text().strip()\n",
    "\n",
    "                                if cleaned_text:\n",
    "                                    reviewers.append(cleaned_text)\n",
    "\n",
    "                            data2['Reviewers'].append('|'.join(reviewers[1:]))\n",
    "            \n",
    "                    \n",
    "                        else:\n",
    "                        \n",
    "                            data2['Reviewers'].append('')\n",
    "\n",
    "                         # Getting Review Title\n",
    "                        reviews_attr = soup_2.select('a[data-hook=\"review-title\"] span')\n",
    "\n",
    "                        reviews = []\n",
    "                        rating_keyword = \"out of 5 stars\"\n",
    "    \n",
    "                        if reviews_attr:\n",
    "                            for i in reviews_attr:\n",
    "\n",
    "                                cleaned_titles = i.get_text().strip()\n",
    "\n",
    "                                if cleaned_titles and rating_keyword not in cleaned_titles:\n",
    "                                    reviews.append(cleaned_titles)\n",
    "\n",
    "                            data2['Reviews'].append('|'.join(reviews))\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            data2['Reviews'].append('')\n",
    "                            \n",
    "                    driver.back()\n",
    "                    sleep(1)\n",
    "\n",
    "                else:\n",
    "                    data2['Product_Link'].append('')\n",
    "        \n",
    "         # Scrolling the page\n",
    "        scroll_position = driver.execute_script(\"return window.pageYOffset + window.innerHeight + window.innerHeight * {0}\".format(increment))\n",
    "    \n",
    "         # Scroll by the incremented amount\n",
    "        driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "\n",
    "         # Check and click the \"Load More\" button\n",
    "        try:\n",
    "            load_more_button = driver.find_element(By.CSS_SELECTOR, 'button[data-testid=\"load-more-view-more-button\"]')\n",
    "\n",
    "            if load_more_button:\n",
    "                \n",
    "                load_more_button.click()\n",
    "                print(\"Button clicked.\")\n",
    "                sleep(5.5)  # Wait for content to load\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            print(\"Load More button not found. Continuing scroll.\")\n",
    "\n",
    "        \n",
    "\n",
    "        if len(productID_set) >= 2000 : # break the loop when distinct data = 2000\n",
    "            break\n",
    "        \n",
    "        \n",
    "                \n",
    "except WebDriverException as web_err:\n",
    "    print(f'Selenium WebDriver error occurred: {web_err}')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the set to a text file, treating None values explicitly\n",
    "with open('productid.txt', 'w') as file:\n",
    "    for id in productID_set:\n",
    "        file.write(str(id) + '\\n')  # Convert None to string and write to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Amazon_data1.json', mode='w', encoding='utf-8') as file:\n",
    "    # Write the data2 dictionary to the file as JSON\n",
    "    json.dump(data2, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
